{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":182633,"sourceType":"datasetVersion","datasetId":78313},{"sourceId":122706,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":103268,"modelId":127499}],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.onnx\nimport torch.nn as nn\nimport torch.optim as optim\nimport onnx\nimport torch.nn.functional as F\nimport tensorflow as tf\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader, Subset\nfrom sklearn.model_selection import train_test_split\nfrom PIL import Image\nimport os\nimport shutil\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-09-29T09:46:15.091950Z","iopub.execute_input":"2024-09-29T09:46:15.092259Z","iopub.status.idle":"2024-09-29T09:46:32.601786Z","shell.execute_reply.started":"2024-09-29T09:46:15.092220Z","shell.execute_reply":"2024-09-29T09:46:32.600997Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# The path to the main directory \nmain_dir = '/kaggle/input/new-plant-diseases-dataset/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/train'  \n\n# List all classes in the main directory\nclasses = [d for d in os.listdir(main_dir) if os.path.isdir(os.path.join(main_dir, d))]\n\n# Print the names of the classes\nprint(\"Classes:\")\nfor class_name in classes:\n    print(class_name)","metadata":{"execution":{"iopub.status.busy":"2024-09-29T09:46:37.250115Z","iopub.execute_input":"2024-09-29T09:46:37.251202Z","iopub.status.idle":"2024-09-29T09:46:37.265790Z","shell.execute_reply.started":"2024-09-29T09:46:37.251006Z","shell.execute_reply":"2024-09-29T09:46:37.264961Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Classes:\nTomato___Late_blight\nTomato___healthy\nGrape___healthy\nOrange___Haunglongbing_(Citrus_greening)\nSoybean___healthy\nSquash___Powdery_mildew\nPotato___healthy\nCorn_(maize)___Northern_Leaf_Blight\nTomato___Early_blight\nTomato___Septoria_leaf_spot\nCorn_(maize)___Cercospora_leaf_spot Gray_leaf_spot\nStrawberry___Leaf_scorch\nPeach___healthy\nApple___Apple_scab\nTomato___Tomato_Yellow_Leaf_Curl_Virus\nTomato___Bacterial_spot\nApple___Black_rot\nBlueberry___healthy\nCherry_(including_sour)___Powdery_mildew\nPeach___Bacterial_spot\nApple___Cedar_apple_rust\nTomato___Target_Spot\nPepper,_bell___healthy\nGrape___Leaf_blight_(Isariopsis_Leaf_Spot)\nPotato___Late_blight\nTomato___Tomato_mosaic_virus\nStrawberry___healthy\nApple___healthy\nGrape___Black_rot\nPotato___Early_blight\nCherry_(including_sour)___healthy\nCorn_(maize)___Common_rust_\nGrape___Esca_(Black_Measles)\nRaspberry___healthy\nTomato___Leaf_Mold\nTomato___Spider_mites Two-spotted_spider_mite\nPepper,_bell___Bacterial_spot\nCorn_(maize)___healthy\n","output_type":"stream"}]},{"cell_type":"code","source":"# List the classes to work on\n# subdirs_to_copy = ['Grape___healthy', 'Corn_(maize)___Northern_Leaf_Blight', 'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot','Grape___Leaf_blight_(Isariopsis_Leaf_Spot)', 'Grape___Black_rot','Corn_(maize)___Common_rust_','Grape___Esca_(Black_Measles)','Corn_(maize)___healthy']  \nsubdirs_to_copy = classes\n# Create a new directory in the Kaggle working directory\nsubset_dir = '/kaggle/working/new_directory'\nos.makedirs(subset_dir, exist_ok=True)\n\n# Copy the selected classes to the new directory\nfor subdir_name in subdirs_to_copy:\n    src_dir_path = os.path.join(main_dir, subdir_name)\n    dest_dir_path = os.path.join(subset_dir, subdir_name)\n    shutil.copytree(src_dir_path, dest_dir_path)\n\nprint(f\"Subdirectories {subdirs_to_copy} copied to {subset_dir}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-29T09:46:41.745162Z","iopub.execute_input":"2024-09-29T09:46:41.745832Z","iopub.status.idle":"2024-09-29T09:55:18.583713Z","shell.execute_reply.started":"2024-09-29T09:46:41.745786Z","shell.execute_reply":"2024-09-29T09:55:18.582703Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Subdirectories ['Tomato___Late_blight', 'Tomato___healthy', 'Grape___healthy', 'Orange___Haunglongbing_(Citrus_greening)', 'Soybean___healthy', 'Squash___Powdery_mildew', 'Potato___healthy', 'Corn_(maize)___Northern_Leaf_Blight', 'Tomato___Early_blight', 'Tomato___Septoria_leaf_spot', 'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot', 'Strawberry___Leaf_scorch', 'Peach___healthy', 'Apple___Apple_scab', 'Tomato___Tomato_Yellow_Leaf_Curl_Virus', 'Tomato___Bacterial_spot', 'Apple___Black_rot', 'Blueberry___healthy', 'Cherry_(including_sour)___Powdery_mildew', 'Peach___Bacterial_spot', 'Apple___Cedar_apple_rust', 'Tomato___Target_Spot', 'Pepper,_bell___healthy', 'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)', 'Potato___Late_blight', 'Tomato___Tomato_mosaic_virus', 'Strawberry___healthy', 'Apple___healthy', 'Grape___Black_rot', 'Potato___Early_blight', 'Cherry_(including_sour)___healthy', 'Corn_(maize)___Common_rust_', 'Grape___Esca_(Black_Measles)', 'Raspberry___healthy', 'Tomato___Leaf_Mold', 'Tomato___Spider_mites Two-spotted_spider_mite', 'Pepper,_bell___Bacterial_spot', 'Corn_(maize)___healthy'] copied to /kaggle/working/new_directory\n","output_type":"stream"}]},{"cell_type":"code","source":"# Initialize a counter for the number of images\nimage_count = 0\n\n# Iterate over all classes and count the images\nfor subdir, _, files in os.walk(subset_dir):\n    image_files = [f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp', '.tiff'))]\n    image_count += len(image_files)\n\nprint(f\"Total number of images: {image_count}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-29T09:55:24.587495Z","iopub.execute_input":"2024-09-29T09:55:24.588367Z","iopub.status.idle":"2024-09-29T09:55:24.713538Z","shell.execute_reply.started":"2024-09-29T09:55:24.588323Z","shell.execute_reply":"2024-09-29T09:55:24.712586Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Total number of images: 70295\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### data preprocessing","metadata":{}},{"cell_type":"code","source":"dataset_path = '/kaggle/working/new_directory'","metadata":{"execution":{"iopub.status.busy":"2024-09-29T09:55:35.484699Z","iopub.execute_input":"2024-09-29T09:55:35.485086Z","iopub.status.idle":"2024-09-29T09:55:35.489651Z","shell.execute_reply.started":"2024-09-29T09:55:35.485048Z","shell.execute_reply":"2024-09-29T09:55:35.488591Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((150, 150)),              # resize all images to 150x150 pixels\n    transforms.ToTensor(),                      # convert images to PyTorch tensors\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])","metadata":{"execution":{"iopub.status.busy":"2024-09-29T09:55:39.472894Z","iopub.execute_input":"2024-09-29T09:55:39.473636Z","iopub.status.idle":"2024-09-29T09:55:39.479029Z","shell.execute_reply.started":"2024-09-29T09:55:39.473596Z","shell.execute_reply":"2024-09-29T09:55:39.477991Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"dataset = datasets.ImageFolder(dataset_path, transform=transform)","metadata":{"execution":{"iopub.status.busy":"2024-09-29T09:55:46.855267Z","iopub.execute_input":"2024-09-29T09:55:46.855695Z","iopub.status.idle":"2024-09-29T09:55:47.185753Z","shell.execute_reply.started":"2024-09-29T09:55:46.855647Z","shell.execute_reply":"2024-09-29T09:55:47.185012Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"dataset.classes","metadata":{"execution":{"iopub.status.busy":"2024-09-29T10:26:28.458042Z","iopub.execute_input":"2024-09-29T10:26:28.458415Z","iopub.status.idle":"2024-09-29T10:26:28.465624Z","shell.execute_reply.started":"2024-09-29T10:26:28.458379Z","shell.execute_reply":"2024-09-29T10:26:28.464714Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"['Apple___Apple_scab',\n 'Apple___Black_rot',\n 'Apple___Cedar_apple_rust',\n 'Apple___healthy',\n 'Blueberry___healthy',\n 'Cherry_(including_sour)___Powdery_mildew',\n 'Cherry_(including_sour)___healthy',\n 'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot',\n 'Corn_(maize)___Common_rust_',\n 'Corn_(maize)___Northern_Leaf_Blight',\n 'Corn_(maize)___healthy',\n 'Grape___Black_rot',\n 'Grape___Esca_(Black_Measles)',\n 'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)',\n 'Grape___healthy',\n 'Orange___Haunglongbing_(Citrus_greening)',\n 'Peach___Bacterial_spot',\n 'Peach___healthy',\n 'Pepper,_bell___Bacterial_spot',\n 'Pepper,_bell___healthy',\n 'Potato___Early_blight',\n 'Potato___Late_blight',\n 'Potato___healthy',\n 'Raspberry___healthy',\n 'Soybean___healthy',\n 'Squash___Powdery_mildew',\n 'Strawberry___Leaf_scorch',\n 'Strawberry___healthy',\n 'Tomato___Bacterial_spot',\n 'Tomato___Early_blight',\n 'Tomato___Late_blight',\n 'Tomato___Leaf_Mold',\n 'Tomato___Septoria_leaf_spot',\n 'Tomato___Spider_mites Two-spotted_spider_mite',\n 'Tomato___Target_Spot',\n 'Tomato___Tomato_Yellow_Leaf_Curl_Virus',\n 'Tomato___Tomato_mosaic_virus',\n 'Tomato___healthy']"},"metadata":{}}]},{"cell_type":"markdown","source":"### train test split \n","metadata":{}},{"cell_type":"code","source":"train_idx, test_idx = train_test_split(list(range(len(dataset))), test_size=0.3, random_state=42)\n\ntrain_dataset = Subset(dataset, train_idx)\ntest_dataset = Subset(dataset, test_idx)","metadata":{"execution":{"iopub.status.busy":"2024-09-29T09:55:55.893457Z","iopub.execute_input":"2024-09-29T09:55:55.894354Z","iopub.status.idle":"2024-09-29T09:55:55.920033Z","shell.execute_reply.started":"2024-09-29T09:55:55.894311Z","shell.execute_reply":"2024-09-29T09:55:55.919294Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"trainloader = DataLoader(train_dataset, batch_size=15, shuffle=True)\ntestloader = DataLoader(test_dataset, batch_size=15, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-09-29T09:55:59.631406Z","iopub.execute_input":"2024-09-29T09:55:59.632048Z","iopub.status.idle":"2024-09-29T09:55:59.636677Z","shell.execute_reply.started":"2024-09-29T09:55:59.632009Z","shell.execute_reply":"2024-09-29T09:55:59.635724Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### define cnn model","metadata":{}},{"cell_type":"code","source":"class CNN_Classification(nn.Module):\n    def __init__(self):\n        super(CNN_Classification, self).__init__()\n\n        # Convolutional Layers\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)       # first convolutional layers\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)            # first pooling layer\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)      # second convolutional layer\n        self.fc1 = nn.Linear(64 * 37 * 37, 128)                                 # 37 x 37 is the size after pooling\n        self.fc2 = nn.Linear(128, len(dataset.classes))\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = torch.flatten(x, 1)                                                 # flatten all dimensions\n        x = torch.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-09-29T09:56:03.580465Z","iopub.execute_input":"2024-09-29T09:56:03.581603Z","iopub.status.idle":"2024-09-29T09:56:03.591448Z","shell.execute_reply.started":"2024-09-29T09:56:03.581558Z","shell.execute_reply":"2024-09-29T09:56:03.590556Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"CNN_Model = CNN_Classification()","metadata":{"execution":{"iopub.status.busy":"2024-09-29T09:56:06.970431Z","iopub.execute_input":"2024-09-29T09:56:06.970925Z","iopub.status.idle":"2024-09-29T09:56:07.093822Z","shell.execute_reply.started":"2024-09-29T09:56:06.970888Z","shell.execute_reply":"2024-09-29T09:56:07.092787Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(CNN_Model.parameters(), lr=0.001)          # Defines the optimizer, Adam with a learning rate of 0.001","metadata":{"execution":{"iopub.status.busy":"2024-09-29T09:56:10.935495Z","iopub.execute_input":"2024-09-29T09:56:10.936378Z","iopub.status.idle":"2024-09-29T09:56:10.940889Z","shell.execute_reply.started":"2024-09-29T09:56:10.936334Z","shell.execute_reply":"2024-09-29T09:56:10.939906Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### train the model","metadata":{}},{"cell_type":"code","source":"num_epochs = 10\nfor epoch in range(num_epochs):\n\n    for images, labels in trainloader:\n        optimizer.zero_grad()\n        outputs = CNN_Model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n\n\nprint('Finished Training')","metadata":{"execution":{"iopub.status.busy":"2024-09-28T09:19:38.252776Z","iopub.execute_input":"2024-09-28T09:19:38.253629Z","iopub.status.idle":"2024-09-28T12:52:12.643545Z","shell.execute_reply.started":"2024-09-28T09:19:38.253583Z","shell.execute_reply":"2024-09-28T12:52:12.642575Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Epoch 1/10, Loss: 0.8185338973999023\nEpoch 2/10, Loss: 0.15399205684661865\nEpoch 3/10, Loss: 0.6147739291191101\nEpoch 4/10, Loss: 0.23931922018527985\nEpoch 5/10, Loss: 0.013808810152113438\nEpoch 6/10, Loss: 0.017673185095191002\nEpoch 7/10, Loss: 1.927212679220247e-06\nEpoch 8/10, Loss: 0.03848181292414665\nEpoch 9/10, Loss: 2.4119392037391663e-05\nEpoch 10/10, Loss: 2.8013973860652186e-06\nFinished Training\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### evaluation","metadata":{}},{"cell_type":"code","source":"import torch\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\n# Switch to evaluation mode\nCNN_Model.eval()\n\n# Initialize variables for storing predictions and true labels\nall_preds = []\nall_labels = []\n\n# Correct and total for accuracy\ncorrect = 0\ntotal = 0\n\n# No gradients needed during evaluation\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n      \n        # Forward pass to get predictions\n        outputs = CNN_Model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        \n        # Update accuracy calculations\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n        \n        # Collect predictions and true labels for other metrics\n        all_preds.extend(predicted.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n# Calculate accuracy\naccuracy = 100 * correct / total\nprint(f'Accuracy: {accuracy:.2f} %')\n\n# Calculate precision, recall, and F1-score using sklearn\nprecision = precision_score(all_labels, all_preds, average='weighted')\nrecall = recall_score(all_labels, all_preds, average='weighted')\nf1 = f1_score(all_labels, all_preds, average='weighted')\n\nprint(f'Precision: {precision * 100:.4f}%')\nprint(f'Recall: {recall * 100:.4f}%')\nprint(f'F1-score: {f1 * 100:.4f}%')","metadata":{"execution":{"iopub.status.busy":"2024-09-28T12:55:09.076879Z","iopub.execute_input":"2024-09-28T12:55:09.077238Z","iopub.status.idle":"2024-09-28T12:58:05.474032Z","shell.execute_reply.started":"2024-09-28T12:55:09.077177Z","shell.execute_reply":"2024-09-28T12:58:05.473096Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Accuracy: 87.56 %\nPrecision: 88.0553%\nRecall: 87.5622%\nF1-score: 87.6052%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Prediction\n","metadata":{}},{"cell_type":"code","source":"# Load a test image and preprocess it\nimg = Image.open('/kaggle/input/new-plant-diseases-dataset/test/test/AppleCedarRust1.JPG')\nimg = transform(img).unsqueeze(0)  # add batch dimension\n\n# Pass the image through the model\nCNN_Model.eval()\noutput = CNN_Model(img)\n_, predicted = torch.max(output, 1)\nprint(f'Predicted class: {dataset.classes[predicted.item()]}')","metadata":{"execution":{"iopub.status.busy":"2024-09-29T10:04:27.436341Z","iopub.execute_input":"2024-09-29T10:04:27.436727Z","iopub.status.idle":"2024-09-29T10:04:27.463810Z","shell.execute_reply.started":"2024-09-29T10:04:27.436680Z","shell.execute_reply":"2024-09-29T10:04:27.462759Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Predicted class: Apple___Cedar_apple_rust\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load a test image and preprocess it\nimg = Image.open('/kaggle/input/new-plant-diseases-dataset/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/valid/Grape___Esca_(Black_Measles)/02af0429-46c1-444b-bf62-a4d0198141e8___FAM_B.Msls 1062.JPG')\nimg = transform(img).unsqueeze(0)  # add batch dimension\n\n# Pass the image through the model\nCNN_Model.eval()\noutput = CNN_Model(img)\n_, predicted = torch.max(output, 1)\nprint(f'Predicted class: {dataset.classes[predicted.item()]}')","metadata":{"execution":{"iopub.status.busy":"2024-09-28T13:00:33.566115Z","iopub.execute_input":"2024-09-28T13:00:33.566519Z","iopub.status.idle":"2024-09-28T13:00:33.595334Z","shell.execute_reply.started":"2024-09-28T13:00:33.566478Z","shell.execute_reply":"2024-09-28T13:00:33.594435Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Predicted class: Grape___Esca_(Black_Measles)\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(CNN_Model.state_dict(), 'cnn_model.pth')  ","metadata":{"execution":{"iopub.status.busy":"2024-09-28T13:01:32.795984Z","iopub.execute_input":"2024-09-28T13:01:32.796715Z","iopub.status.idle":"2024-09-28T13:01:32.854098Z","shell.execute_reply.started":"2024-09-28T13:01:32.796666Z","shell.execute_reply":"2024-09-28T13:01:32.853340Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"CNN_Model.load_state_dict(torch.load('/kaggle/input/cnn/pytorch/default/1/cnn_model.pth'))","metadata":{"execution":{"iopub.status.busy":"2024-09-29T10:02:02.592393Z","iopub.execute_input":"2024-09-29T10:02:02.593296Z","iopub.status.idle":"2024-09-29T10:02:03.084182Z","shell.execute_reply.started":"2024-09-29T10:02:02.593250Z","shell.execute_reply":"2024-09-29T10:02:03.083235Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/3576470478.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  CNN_Model.load_state_dict(torch.load('/kaggle/input/cnn/pytorch/default/1/cnn_model.pth'))\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Pretrained models","metadata":{}},{"cell_type":"markdown","source":"### Vgg16","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import models\n\n# Check if GPU is available and set device accordingly\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f'Using device: {device}')\n\n# Load pre-trained VGG16 model\nvgg16 = models.vgg16(pretrained=True)\n\n# Transfer model to the GPU\nvgg16 = vgg16.to(device)\n\n# Freeze all the layers (optional, if you don't want to train the convolutional layers)\nfor param in vgg16.parameters():\n    param.requires_grad = False\n\n# Modify the classifier to fit the number of classes in your dataset\nvgg16.classifier[6] = nn.Linear(4096, len(dataset.classes))  # 4096 is the input to the last layer\n\n# Transfer classifier changes to the GPU\nvgg16.classifier = vgg16.classifier.to(device)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss().to(device)  # Move criterion to GPU\noptimizer = optim.Adam(vgg16.parameters(), lr=0.001)\n\n# Train the model\nnum_epochs = 5\nfor epoch in range(num_epochs):\n    for images, labels in trainloader:\n        # Transfer images and labels to GPU\n        images, labels = images.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = vgg16(images)\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n\n    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n\nprint('Finished Training VGG16')","metadata":{"execution":{"iopub.status.busy":"2024-09-28T13:06:31.782457Z","iopub.execute_input":"2024-09-28T13:06:31.783305Z","iopub.status.idle":"2024-09-28T13:15:15.807236Z","shell.execute_reply.started":"2024-09-28T13:06:31.783260Z","shell.execute_reply":"2024-09-28T13:15:15.806260Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n100%|██████████| 528M/528M [00:02<00:00, 216MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5, Loss: 2.8841552734375\nEpoch 2/5, Loss: 2.183500051498413\nEpoch 3/5, Loss: 1.166042447090149\nEpoch 4/5, Loss: 1.3930320739746094\nEpoch 5/5, Loss: 0.9436690211296082\nFinished Training VGG16\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\n# Switch to evaluation mode\nvgg16.eval()\n\n# Initialize variables for storing predictions and true labels\nall_preds = []\nall_labels = []\n\n# Correct and total for accuracy\ncorrect = 0\ntotal = 0\n\n# No gradients needed during evaluation\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n      \n        # Forward pass to get predictions\n        outputs = vgg16(images.to(device))\n        _, predicted = torch.max(outputs.data, 1)\n        \n        # Update accuracy calculations\n        total += labels.size(0)\n        correct += (predicted == labels.to(device)).sum().item()\n        \n        # Collect predictions and true labels for other metrics\n        all_preds.extend(predicted.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n# Calculate accuracy\naccuracy = 100 * correct / total\nprint(f'Accuracy: {accuracy:.2f} %')\n\n# Calculate precision, recall, and F1-score using sklearn\nprecision = precision_score(all_labels, all_preds, average='weighted')\nrecall = recall_score(all_labels, all_preds, average='weighted')\nf1 = f1_score(all_labels, all_preds, average='weighted')\n\nprint(f'Precision: {precision * 100:.4f}%')\nprint(f'Recall: {recall * 100:.4f}%')\nprint(f'F1-score: {f1 * 100:.4f}%')","metadata":{"execution":{"iopub.status.busy":"2024-09-28T13:15:49.513295Z","iopub.execute_input":"2024-09-28T13:15:49.514108Z","iopub.status.idle":"2024-09-28T13:17:01.522162Z","shell.execute_reply.started":"2024-09-28T13:15:49.514069Z","shell.execute_reply":"2024-09-28T13:17:01.521228Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Accuracy: 85.32 %\nPrecision: 86.8432%\nRecall: 85.3241%\nF1-score: 85.3637%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### AlexNet","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import models\n\n# Check if GPU is available and set device accordingly\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f'Using device: {device}')\n\n# Load pre-trained AlexNet model\nalexnet = models.alexnet(pretrained=True)\n\n# Transfer model to the GPU\nalexnet = alexnet.to(device)\n\n# Freeze layers if desired\nfor param in alexnet.parameters():\n    param.requires_grad = False\n\n# Modify the classifier to fit the number of classes in your dataset\nalexnet.classifier[6] = nn.Linear(4096, len(dataset.classes))\n\n# Transfer classifier changes to the GPU\nalexnet.classifier = alexnet.classifier.to(device)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss().to(device)  # Move loss function to GPU\noptimizer = optim.Adam(alexnet.parameters(), lr=0.02)\n\n# Train the model\nnum_epochs = 5\nfor epoch in range(num_epochs):\n    for images, labels in trainloader:\n        # Transfer images and labels to GPU\n        images, labels = images.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = alexnet(images)\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n\n    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n\nprint('Finished Training AlexNet')","metadata":{"execution":{"iopub.status.busy":"2024-09-29T11:00:03.458139Z","iopub.execute_input":"2024-09-29T11:00:03.458927Z","iopub.status.idle":"2024-09-29T11:08:10.325598Z","shell.execute_reply.started":"2024-09-29T11:00:03.458885Z","shell.execute_reply":"2024-09-29T11:08:10.324572Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Using device: cuda\nEpoch 1/5, Loss: 36.55414962768555\nEpoch 2/5, Loss: 0.0\nEpoch 3/5, Loss: 28.4639892578125\nEpoch 4/5, Loss: 0.0\nEpoch 5/5, Loss: 34.99062728881836\nFinished Training AlexNet\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\n# Switch to evaluation model\nalexnet.eval()\n\n# Initialize variables for storing predictions and true labels\nall_preds = []\nall_labels = []\n\n# Correct and total for accuracy\ncorrect = 0\ntotal = 0\n\n# No gradients needed during evaluation\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n      \n        # Forward pass to get predictions\n        outputs = alexnet(images.to(device))\n        _, predicted = torch.max(outputs.data, 1)\n        \n        # Update accuracy calculations\n        total += labels.size(0)\n        correct += (predicted == labels.to(device)).sum().item()\n        \n        # Collect predictions and true labels for other metrics\n        all_preds.extend(predicted.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n# Calculate accuracy\naccuracy = 100 * correct / total\nprint(f'Accuracy: {accuracy:.2f} %')\n\n# Calculate precision, recall, and F1-score using sklearn\nprecision = precision_score(all_labels, all_preds, average='weighted')\nrecall = recall_score(all_labels, all_preds, average='weighted')\nf1 = f1_score(all_labels, all_preds, average='weighted')\n\nprint(f'Precision: {precision * 100:.2f}%')\nprint(f'Recall: {recall * 100:.2f}%')\nprint(f'F1-score: {f1 * 100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-09-29T11:08:21.234726Z","iopub.execute_input":"2024-09-29T11:08:21.235552Z","iopub.status.idle":"2024-09-29T11:09:04.482545Z","shell.execute_reply.started":"2024-09-29T11:08:21.235513Z","shell.execute_reply":"2024-09-29T11:09:04.481482Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Accuracy: 87.12 %\nPrecision: 88.29%\nRecall: 87.12%\nF1-score: 87.15%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### DenseNet","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import models\n\n# Check if GPU is available and set device accordingly\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f'Using device: {device}')\n\n# Load pre-trained DenseNet121 model\ndensenet = models.densenet121(pretrained=True)\n\n# Transfer model to the GPU\ndensenet = densenet.to(device)\n\n# Freeze all the layers if you don't want to train the convolutional layers (optional)\nfor param in densenet.parameters():\n    param.requires_grad = False\n\n# Modify the classifier to fit the number of classes in your dataset\nnum_ftrs = densenet.classifier.in_features\ndensenet.classifier = nn.Linear(num_ftrs, len(dataset.classes))\n\n# Transfer classifier changes to the GPU\ndensenet.classifier = densenet.classifier.to(device)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss().to(device)  # Move loss function to GPU\noptimizer = optim.Adam(densenet.parameters(), lr=0.02)\n\n# Train the model\nnum_epochs = 4\nfor epoch in range(num_epochs):\n    for images, labels in trainloader:\n        # Transfer images and labels to GPU\n        images, labels = images.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = densenet(images)\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n\n    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n\nprint('Finished Training DenseNet')","metadata":{"execution":{"iopub.status.busy":"2024-09-29T11:45:26.495340Z","iopub.execute_input":"2024-09-29T11:45:26.496006Z","iopub.status.idle":"2024-09-29T11:56:32.493092Z","shell.execute_reply.started":"2024-09-29T11:45:26.495966Z","shell.execute_reply":"2024-09-29T11:56:32.491973Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/4, Loss: 3.839745283126831\nEpoch 2/4, Loss: 0.0\nEpoch 3/4, Loss: 5.495326519012451\nEpoch 4/4, Loss: 5.477962970733643\nFinished Training DenseNet\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\n# Switch to evaluation model\n# densenet.eval()\n\n# Initialize variables for storing predictions and true labels\nall_preds = []\nall_labels = []\n\n# Correct and total for accuracy\ncorrect = 0\ntotal = 0\n\n# No gradients needed during evaluation\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n      \n        # Forward pass to get predictions\n        outputs = densenet(images.to(device))\n        _, predicted = torch.max(outputs.data, 1)\n        \n        # Update accuracy calculations\n        total += labels.size(0)\n        correct += (predicted == labels.to(device)).sum().item()\n        \n        # Collect predictions and true labels for other metrics\n        all_preds.extend(predicted.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n# Calculate accuracy\naccuracy = 100 * correct / total\nprint(f'Accuracy: {accuracy:.2f} %')\n\n# Calculate precision, recall, and F1-score using sklearn\nprecision = precision_score(all_labels, all_preds, average='weighted')\nrecall = recall_score(all_labels, all_preds, average='weighted')\nf1 = f1_score(all_labels, all_preds, average='weighted')\n\nprint(f'Precision: {precision * 100:.2f}%')\nprint(f'Recall: {recall * 100:.2f}%')\nprint(f'F1-score: {f1 * 100:.2f}%')\n","metadata":{"execution":{"iopub.status.busy":"2024-09-29T11:56:39.372918Z","iopub.execute_input":"2024-09-29T11:56:39.373778Z","iopub.status.idle":"2024-09-29T11:57:51.584408Z","shell.execute_reply.started":"2024-09-29T11:56:39.373723Z","shell.execute_reply":"2024-09-29T11:57:51.583435Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Accuracy: 87.41 %\nPrecision: 89.72%\nRecall: 87.41%\nF1-score: 87.76%\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(vgg16.state_dict(), 'vgg16.pth')  \ntorch.save(alexnet.state_dict(), 'alexnet_model.pth') \ntorch.save(densenet.state_dict(), 'densenet.pth')","metadata":{"execution":{"iopub.status.busy":"2024-09-29T11:58:16.966949Z","iopub.execute_input":"2024-09-29T11:58:16.967337Z","iopub.status.idle":"2024-09-29T11:58:17.511374Z","shell.execute_reply.started":"2024-09-29T11:58:16.967300Z","shell.execute_reply":"2024-09-29T11:58:17.510434Z"},"trusted":true},"execution_count":33,"outputs":[]}]}